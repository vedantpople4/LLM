{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO/irV4b4aMX0234h4c2zpd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vedantpople4/LLM/blob/main/Part3_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LMM3gLHYxAc",
        "outputId": "bb7bf70a-f900-421c-cb11-f2196fe70fcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.17.1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "import tensorflow.keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import random"
      ],
      "metadata": {
        "id": "A9t3vS50lf2n"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Tokenizer (same as before)\n",
        "class SimpleTokenizer:\n",
        "    def __init__(self):\n",
        "        self.vocab = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
        "        self.reverse_vocab = {0: '<pad>', 1: '<sos>', 2: '<eos>', 3: '<unk>'}\n",
        "\n",
        "    def fit(self, texts):\n",
        "        words = set(\" \".join(texts).split())\n",
        "        for word in words:\n",
        "            if word not in self.vocab:\n",
        "                self.vocab[word] = len(self.vocab)\n",
        "                self.reverse_vocab[len(self.reverse_vocab)] = word\n",
        "\n",
        "    def encode(self, text):\n",
        "        return [self.vocab['<sos>']] + [self.vocab.get(word, self.vocab['<unk>']) for word in text.split()] + [self.vocab['<eos>']]\n",
        "\n",
        "    def decode(self, indices):\n",
        "        return \" \".join([self.reverse_vocab.get(idx, '<unk>') for idx in indices if idx not in [self.vocab['<sos>'], self.vocab['<eos>'], self.vocab['<pad>']]])\n",
        "\n",
        "# Custom Dataset (same as before)\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, src_texts, tgt_texts, src_tokenizer, tgt_tokenizer):\n",
        "        self.src_encoded = [src_tokenizer.encode(text) for text in src_texts]\n",
        "        self.tgt_encoded = [tgt_tokenizer.encode(text) for text in tgt_texts]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_encoded)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.src_encoded[idx], self.tgt_encoded[idx]"
      ],
      "metadata": {
        "id": "E84PRR9dlk9Y"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collate function\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = zip(*batch)\n",
        "\n",
        "    src_lengths = [len(seq) for seq in src_batch]\n",
        "    tgt_lengths = [len(seq) for seq in tgt_batch]\n",
        "\n",
        "    max_src_len = max(src_lengths)\n",
        "    max_tgt_len = max(tgt_lengths)\n",
        "\n",
        "    src_padded = [seq + [src_tokenizer.vocab['<pad>']] * (max_src_len - len(seq)) for seq in src_batch]\n",
        "    tgt_padded = [seq + [tgt_tokenizer.vocab['<pad>']] * (max_tgt_len - len(seq)) for seq in tgt_batch]\n",
        "\n",
        "    return torch.tensor(src_padded), torch.tensor(tgt_padded)"
      ],
      "metadata": {
        "id": "9242Uzp_lyD-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Transformer (same as before)\n",
        "class CustomTransformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers):\n",
        "        super(CustomTransformer, self).__init__()\n",
        "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers)\n",
        "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_embed = self.src_embedding(src).transpose(0, 1)\n",
        "        tgt_embed = self.tgt_embedding(tgt).transpose(0, 1)\n",
        "\n",
        "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_embed.size(0)).to(tgt_embed.device)\n",
        "\n",
        "        output = self.transformer(src_embed, tgt_embed, tgt_mask=tgt_mask)\n",
        "        return self.fc_out(output.transpose(0, 1))"
      ],
      "metadata": {
        "id": "wTNUmP65l1P2"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Toy dataset (same as before)\n",
        "eng_texts = [\n",
        "    \"hello how are you\",\n",
        "    \"i love programming\",\n",
        "    \"machine learning is fun\",\n",
        "    \"python is a great language\",\n",
        "    \"deep learning is powerful\"\n",
        "]\n",
        "\n",
        "fr_texts = [\n",
        "    \"bonjour comment allez vous\",\n",
        "    \"j'aime la programmation\",\n",
        "    \"l'apprentissage automatique est amusant\",\n",
        "    \"python est un excellent langage\",\n",
        "    \"l'apprentissage profond est puissant\"\n",
        "]"
      ],
      "metadata": {
        "id": "U7JXdhpsl74y"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "src_tokenizer = SimpleTokenizer()\n",
        "tgt_tokenizer = SimpleTokenizer()\n",
        "\n",
        "src_tokenizer.fit(eng_texts)\n",
        "tgt_tokenizer.fit(fr_texts)\n",
        "\n",
        "# Create dataset and dataloader\n",
        "dataset = CustomDataset(eng_texts, fr_texts, src_tokenizer, tgt_tokenizer)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# Initialize model\n",
        "src_vocab_size = len(src_tokenizer.vocab)\n",
        "tgt_vocab_size = len(tgt_tokenizer.vocab)\n",
        "d_model = 32\n",
        "nhead = 2\n",
        "num_encoder_layers = 2\n",
        "num_decoder_layers = 2"
      ],
      "metadata": {
        "id": "0RCtj_cml_L2"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomTransformer(src_vocab_size, tgt_vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers)\n",
        "\n",
        "# Training loop\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=src_tokenizer.vocab['<pad>'])\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for src, tgt in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt[:, :-1])\n",
        "        loss = criterion(output.reshape(-1, tgt_vocab_size), tgt[:, 1:].reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader):.4f}\")\n",
        "\n",
        "# Test the model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sl0fTSzBgJEx",
        "outputId": "89f3c757-9556-4b2f-9824-4c98828da374"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/100, Loss: 1.0845\n",
            "Epoch 20/100, Loss: 0.6032\n",
            "Epoch 30/100, Loss: 0.3345\n",
            "Epoch 40/100, Loss: 0.2421\n",
            "Epoch 50/100, Loss: 0.1677\n",
            "Epoch 60/100, Loss: 0.1283\n",
            "Epoch 70/100, Loss: 0.1021\n",
            "Epoch 80/100, Loss: 0.0768\n",
            "Epoch 90/100, Loss: 0.0661\n",
            "Epoch 100/100, Loss: 0.0599\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "test_sentence = \"deep learning is fun\"\n",
        "src_tensor = torch.tensor(src_tokenizer.encode(test_sentence)).unsqueeze(0)\n",
        "tgt_tensor = torch.tensor([tgt_tokenizer.vocab['<sos>']]).unsqueeze(0)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for _ in range(10):\n",
        "        output = model(src_tensor, tgt_tensor)\n",
        "        next_word = output.argmax(2)[:, -1]\n",
        "        tgt_tensor = torch.cat([tgt_tensor, next_word.unsqueeze(0)], dim=1)\n",
        "\n",
        "        if next_word.item() == tgt_tokenizer.vocab['<eos>']:\n",
        "            break\n",
        "\n",
        "translated = tgt_tokenizer.decode(tgt_tensor.squeeze().tolist())\n",
        "print(f\"Input: {test_sentence}\")\n",
        "print(f\"Translation: {translated}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBJPVJIIkpZo",
        "outputId": "eb6cbcde-a40a-4c57-99d3-50359eba110f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: deep learning is fun\n",
            "Translation: l'apprentissage automatique est amusant\n"
          ]
        }
      ]
    }
  ]
}